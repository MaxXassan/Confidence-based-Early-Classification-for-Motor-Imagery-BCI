{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data - \\n    - 2 sessions, \\n    - 6 runs in each session, \\n    - 12*4(=48) trials in each run,\\n    - 288 trials in each session.\\n    - 25 channels - (first 22 are EEG, last 3 are EOG) - need to remove those\\n    - cue onset classes and event type values: in training data 1, 2, 3, 4 -> 769, 770, 771, 772\\n    - trials containing artifacts as scored by experts are marked as events\\n      with the type 1023\\n\\n#first subject, training session\\n#print(os.listdir())\\n##for py files, its in 'Notebook/...\\nfile = sio.loadmat('A01T.mat')\\n#print(file)\\n#9 runs: 3 eog, 6 eeg\\ndata = file['data'][0]\\n\\n#remove the 3 first eog runs of each sesssion\\ndata = data[3:]\\ndata = data[0][0][0]\\n\\nprint(data[0])\\n\\n##maybe provide list of the actual channel names(check: https://mne.tools/stable/auto_tutorials/simulation/10_array_objs.html#sphx-glr-auto-tutorials-simulation-10-array-objs-py)\\n# Extract necessary information\\nnum_channels = 25  # Number of EEG channels - num_channels = len(data[0][0]) # Number of columns in the data matrix\\nsampling_freq = 250  # Sampling frequency -sampling_freq = data[3][0] # 250Hz\\n\\n# Create MNE info instance\\ninfo = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\\n\\nprint(info)\\nprint(data[0].shape)\\nraw_array_1T = RawArray(data[0].T, info=info)\\nraw_array_1T.plot()\\nplt.show()\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import requests\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "\n",
    "#Load the mat file of the first subjects training data\n",
    "'''\n",
    "'''Data - \n",
    "    - 2 sessions, \n",
    "    - 6 runs in each session, \n",
    "    - 12*4(=48) trials in each run,\n",
    "    - 288 trials in each session.\n",
    "    - 25 channels - (first 22 are EEG, last 3 are EOG) - need to remove those\n",
    "    - cue onset classes and event type values: in training data 1, 2, 3, 4 -> 769, 770, 771, 772\n",
    "    - trials containing artifacts as scored by experts are marked as events\n",
    "      with the type 1023\n",
    "\n",
    "#first subject, training session\n",
    "#print(os.listdir())\n",
    "##for py files, its in 'Notebook/...\n",
    "file = sio.loadmat('A01T.mat')\n",
    "#print(file)\n",
    "#9 runs: 3 eog, 6 eeg\n",
    "data = file['data'][0]\n",
    "\n",
    "#remove the 3 first eog runs of each sesssion\n",
    "data = data[3:]\n",
    "data = data[0][0][0]\n",
    "\n",
    "print(data[0])\n",
    "\n",
    "##maybe provide list of the actual channel names(check: https://mne.tools/stable/auto_tutorials/simulation/10_array_objs.html#sphx-glr-auto-tutorials-simulation-10-array-objs-py)\n",
    "# Extract necessary information\n",
    "num_channels = 25  # Number of EEG channels - num_channels = len(data[0][0]) # Number of columns in the data matrix\n",
    "sampling_freq = 250  # Sampling frequency -sampling_freq = data[3][0] # 250Hz\n",
    "\n",
    "# Create MNE info instance\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\n",
    "\n",
    "print(info)\n",
    "print(data[0].shape)\n",
    "raw_array_1T = RawArray(data[0].T, info=info)\n",
    "raw_array_1T.plot()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot EEG data using matplotlib\\n# Plot EEG data using stacked plots\\nnum_channels = len(data[0][0])\\nprint(num_channels)\\ntime = np.arange(data[0].shape[0] / 250.0)  # Assuming 250 Hz sampling frequency\\nprint(time)\\n\\nplt.figure(figsize=(12, 8))\\nfor i in range(num_channels):\\n    plt.subplot(num_channels, 1, i+1)\\n    plt.plot(time, data[:, i])\\n    plt.title('EEG Channel {}'.format(i+1))\\n    plt.xlabel('Time (s)')\\n    plt.ylabel('Amplitude')\\n    plt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rawarray\n",
    "'''import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "from mne import EpochsArray, Epochs\n",
    "\n",
    "# Load the mat file of the first subject's training data\n",
    "file = sio.loadmat('A01T.mat')\n",
    "\n",
    "# Extract EEG data\n",
    "data = file['data'][0]\n",
    "\n",
    "# Remove the first 3 EOG runs of each session\n",
    "data = data[3:]\n",
    "data = data[0][0][0]\n",
    "\n",
    "# Extract necessary information\n",
    "num_channels = 25  # Number of EEG channels\n",
    "sampling_freq = 250  # Sampling frequency\n",
    "\n",
    "# Create MNE info instance\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\n",
    "\n",
    "print(data[0].T)\n",
    "print((data[0]).T.shape)\n",
    "print(data[0][0][0].T)\n",
    "time = (data[0].shape[0] / 250.0) \n",
    "# Assuming 250 Hz sampling frequency\n",
    "# Create RawArray object and plot the data\n",
    "##array for each trial and the concatenate? also need to add montage - electrode positions?\n",
    "raw_array_1T = RawArray(data[0].T, info=info)\n",
    "raw_array_1T.plot(events = data[0].T, n_channels = 25, duration= time, show_scrollbars=False, show_scalebars=False)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "'''\n",
    "# Plot EEG data using matplotlib\n",
    "# Plot EEG data using stacked plots\n",
    "num_channels = len(data[0][0])\n",
    "print(num_channels)\n",
    "time = np.arange(data[0].shape[0] / 250.0)  # Assuming 250 Hz sampling frequency\n",
    "print(time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(num_channels):\n",
    "    plt.subplot(num_channels, 1, i+1)\n",
    "    plt.plot(time, data[:, i])\n",
    "    plt.title('EEG Channel {}'.format(i+1))\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Epochs\\nimport scipy.io as sio\\nfrom moabb.datasets import BNCI2014_001\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport mne\\nfrom mne.io import RawArray\\nfrom mne import EpochsArray\\n\\n# Load the mat file of the first subject\\'s training data\\nfile = sio.loadmat(\\'A01T.mat\\')\\n\\n# Extract EEG data for the first\\ndata = file[\\'data\\'][0]\\n# Remove the first 3 EOG runs of each session\\ndata = data[3:]\\ndata = data[0][0][0]\\n\\n# Extract necessary information\\nnum_channels = 25  # Number of EEG channels\\nsampling_freq = 250  # Sampling frequency\\n\\n# Create MNE info instance\\ninfo = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=[\\'eeg\\']*22 + [\\'eog\\']*3)\\n\\nprint(data[0].T)\\nprint((data[0]).T.shape)\\nprint(data[0][0][0].T)\\ntimeInTrial = (data[0].shape[0] / 250.0) /48\\nsamplesInTrial = data[0].shape[0] // 48\\nprint(\"samplesintrial:\", samplesInTrial)\\n# Assuming 250 Hz sampling frequency\\n\\n# Create RawArray objects and plot the data\\n\\n##best way - find the event type markers - 769, 770,771,772 to identify exact cue onsets - were are they tho?, \\n#then add 4 seconds(4 seconds are 1000 samples, 4*250Hz) to that time/sample to get t= 2 to t =6\\n\\n##array for each trial and the concatenate? also need to add montage - electrode positions?\\n\\nraw_array_1T = RawArray(data[0].T, info=info)\\n\\ndataset = BNCI2014_001\\n\\nsubject_list = [1]\\nsessions = dataset.get_data(subject_list)\\nX, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)\\nepochs, labels, meta = paradigm.get_data(\\n    dataset=dataset, subjects=subject_list, return_epochs=True\\n\\nstart = 0\\nraw_arrays = []\\nfor i in range(48):\\n    print(\"start:\", start)\\n    end = (i+1) * samplesInTrial\\n    print(\"end\", end)\\n    # Slice the data to ensure each slice has the same shape\\n    sliced_data = data[0][start:end].T\\n    raw_arrays.append(RawArray(sliced_data, info=info))\\n    start = end\\n\\nevent_labels = data[2]\\nprint(event_labels) '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Epochs\n",
    "import scipy.io as sio\n",
    "from moabb.datasets import BNCI2014_001\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "from mne import EpochsArray\n",
    "\n",
    "# Load the mat file of the first subject's training data\n",
    "file = sio.loadmat('A01T.mat')\n",
    "\n",
    "# Extract EEG data for the first\n",
    "data = file['data'][0]\n",
    "# Remove the first 3 EOG runs of each session\n",
    "data = data[3:]\n",
    "data = data[0][0][0]\n",
    "\n",
    "# Extract necessary information\n",
    "num_channels = 25  # Number of EEG channels\n",
    "sampling_freq = 250  # Sampling frequency\n",
    "\n",
    "# Create MNE info instance\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\n",
    "\n",
    "print(data[0].T)\n",
    "print((data[0]).T.shape)\n",
    "print(data[0][0][0].T)\n",
    "timeInTrial = (data[0].shape[0] / 250.0) /48\n",
    "samplesInTrial = data[0].shape[0] // 48\n",
    "print(\"samplesintrial:\", samplesInTrial)\n",
    "# Assuming 250 Hz sampling frequency\n",
    "\n",
    "# Create RawArray objects and plot the data\n",
    "\n",
    "##best way - find the event type markers - 769, 770,771,772 to identify exact cue onsets - were are they tho?, \n",
    "#then add 4 seconds(4 seconds are 1000 samples, 4*250Hz) to that time/sample to get t= 2 to t =6\n",
    "\n",
    "##array for each trial and the concatenate? also need to add montage - electrode positions?\n",
    "\n",
    "raw_array_1T = RawArray(data[0].T, info=info)\n",
    "\n",
    "dataset = BNCI2014_001\n",
    "\n",
    "subject_list = [1]\n",
    "sessions = dataset.get_data(subject_list)\n",
    "X, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)\n",
    "epochs, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset, subjects=subject_list, return_epochs=True\n",
    "\n",
    "start = 0\n",
    "raw_arrays = []\n",
    "for i in range(48):\n",
    "    print(\"start:\", start)\n",
    "    end = (i+1) * samplesInTrial\n",
    "    print(\"end\", end)\n",
    "    # Slice the data to ensure each slice has the same shape\n",
    "    sliced_data = data[0][start:end].T\n",
    "    raw_arrays.append(RawArray(sliced_data, info=info))\n",
    "    start = end\n",
    "\n",
    "event_labels = data[2]\n",
    "print(event_labels) '''\n",
    "#48 labels for the 48 trials in this first session\n",
    "\n",
    "#epochs_arrays = EpochsArray(raw_arrays, info=info)\n",
    "#epochs_arrays.plot(picks=\"misc\", show_scrollbars=False, events=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing from all possible events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject_list_dataset: \n",
      " [1, 2, 3]\n",
      "\n",
      "\n",
      "Sessions: \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Sessions: \n",
      " {1: {'0train': {'0': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '1': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '2': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '3': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '4': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '5': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>}, '1test': {'0': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '1': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '2': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '3': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '4': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '5': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>}}}\n",
      "\n",
      "\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "576 matching events found\n",
      "No baseline correction applied\n",
      "epochs: \n",
      " <EpochsArray |  576 events (all good), 2 – 6 s, baseline off, ~96.8 MB, data loaded, with metadata,\n",
      " 'left_hand': 144\n",
      " 'right_hand': 144\n",
      " 'feet': 144\n",
      " 'tongue': 144>\n",
      "\n",
      "\n",
      "labels: \n",
      " ['tongue' 'feet' 'right_hand' 'left_hand' 'left_hand' 'right_hand' 'feet'\n",
      " 'tongue' 'right_hand' 'feet' 'left_hand' 'left_hand' 'left_hand' 'tongue'\n",
      " 'right_hand' 'right_hand' 'left_hand' 'left_hand' 'feet' 'left_hand'\n",
      " 'right_hand' 'tongue' 'tongue' 'feet' 'left_hand' 'tongue' 'tongue'\n",
      " 'right_hand' 'tongue' 'tongue' 'right_hand' 'left_hand' 'right_hand'\n",
      " 'feet' 'feet' 'feet' 'tongue' 'feet' 'left_hand' 'tongue' 'right_hand'\n",
      " 'feet' 'right_hand' 'feet' 'tongue' 'right_hand' 'feet' 'left_hand'\n",
      " 'left_hand' 'left_hand' 'tongue' 'right_hand' 'left_hand' 'feet'\n",
      " 'left_hand' 'feet' 'right_hand' 'tongue' 'left_hand' 'feet' 'feet'\n",
      " 'left_hand' 'feet' 'right_hand' 'tongue' 'tongue' 'tongue' 'feet'\n",
      " 'left_hand' 'tongue' 'right_hand' 'tongue' 'right_hand' 'left_hand'\n",
      " 'feet' 'right_hand' 'left_hand' 'feet' 'feet' 'left_hand' 'feet' 'tongue'\n",
      " 'tongue' 'right_hand' 'left_hand' 'right_hand' 'tongue' 'right_hand'\n",
      " 'tongue' 'feet' 'right_hand' 'right_hand' 'right_hand' 'feet' 'tongue'\n",
      " 'left_hand' 'right_hand' 'tongue' 'left_hand' 'feet' 'feet' 'tongue'\n",
      " 'left_hand' 'left_hand' 'feet' 'right_hand' 'tongue' 'tongue' 'tongue'\n",
      " 'right_hand' 'left_hand' 'feet' 'right_hand' 'tongue' 'left_hand'\n",
      " 'tongue' 'feet' 'right_hand' 'tongue' 'tongue' 'left_hand' 'right_hand'\n",
      " 'right_hand' 'feet' 'tongue' 'right_hand' 'left_hand' 'left_hand'\n",
      " 'tongue' 'right_hand' 'left_hand' 'feet' 'right_hand' 'right_hand' 'feet'\n",
      " 'left_hand' 'tongue' 'feet' 'feet' 'feet' 'feet' 'left_hand' 'right_hand'\n",
      " 'left_hand' 'right_hand' 'left_hand' 'left_hand' 'feet' 'feet'\n",
      " 'right_hand' 'feet' 'tongue' 'left_hand' 'tongue' 'left_hand' 'left_hand'\n",
      " 'right_hand' 'tongue' 'feet' 'right_hand' 'tongue' 'feet' 'tongue' 'feet'\n",
      " 'tongue' 'right_hand' 'right_hand' 'tongue' 'left_hand' 'right_hand'\n",
      " 'right_hand' 'right_hand' 'feet' 'tongue' 'left_hand' 'tongue'\n",
      " 'left_hand' 'feet' 'left_hand' 'tongue' 'left_hand' 'feet' 'left_hand'\n",
      " 'right_hand' 'feet' 'feet' 'tongue' 'left_hand' 'right_hand' 'tongue'\n",
      " 'right_hand' 'feet' 'feet' 'left_hand' 'tongue' 'right_hand' 'tongue'\n",
      " 'left_hand' 'left_hand' 'feet' 'feet' 'right_hand' 'tongue' 'right_hand'\n",
      " 'right_hand' 'left_hand' 'right_hand' 'tongue' 'tongue' 'right_hand'\n",
      " 'right_hand' 'right_hand' 'right_hand' 'tongue' 'tongue' 'feet' 'tongue'\n",
      " 'left_hand' 'right_hand' 'feet' 'right_hand' 'left_hand' 'tongue'\n",
      " 'left_hand' 'tongue' 'left_hand' 'left_hand' 'left_hand' 'left_hand'\n",
      " 'feet' 'feet' 'tongue' 'right_hand' 'feet' 'feet' 'feet' 'tongue' 'feet'\n",
      " 'left_hand' 'feet' 'left_hand' 'tongue' 'right_hand' 'tongue' 'tongue'\n",
      " 'feet' 'tongue' 'tongue' 'feet' 'right_hand' 'tongue' 'feet' 'left_hand'\n",
      " 'right_hand' 'right_hand' 'right_hand' 'feet' 'right_hand' 'tongue'\n",
      " 'feet' 'tongue' 'right_hand' 'feet' 'left_hand' 'tongue' 'left_hand'\n",
      " 'feet' 'tongue' 'left_hand' 'feet' 'left_hand' 'right_hand' 'right_hand'\n",
      " 'left_hand' 'tongue' 'left_hand' 'tongue' 'feet' 'feet' 'left_hand'\n",
      " 'feet' 'right_hand' 'right_hand' 'left_hand' 'feet' 'left_hand'\n",
      " 'right_hand' 'left_hand' 'left_hand' 'right_hand' 'right_hand'\n",
      " 'left_hand' 'right_hand' 'left_hand' 'right_hand' 'feet' 'right_hand'\n",
      " 'tongue' 'left_hand' 'feet' 'right_hand' 'left_hand' 'tongue' 'tongue'\n",
      " 'tongue' 'tongue' 'tongue' 'left_hand' 'feet' 'right_hand' 'left_hand'\n",
      " 'left_hand' 'feet' 'tongue' 'left_hand' 'feet' 'feet' 'feet' 'left_hand'\n",
      " 'right_hand' 'left_hand' 'right_hand' 'right_hand' 'left_hand'\n",
      " 'right_hand' 'feet' 'right_hand' 'feet' 'feet' 'tongue' 'feet' 'feet'\n",
      " 'tongue' 'tongue' 'tongue' 'tongue' 'tongue' 'feet' 'right_hand'\n",
      " 'left_hand' 'left_hand' 'right_hand' 'feet' 'tongue' 'right_hand' 'feet'\n",
      " 'left_hand' 'left_hand' 'left_hand' 'tongue' 'right_hand' 'right_hand'\n",
      " 'left_hand' 'left_hand' 'feet' 'left_hand' 'right_hand' 'tongue' 'tongue'\n",
      " 'feet' 'left_hand' 'tongue' 'tongue' 'right_hand' 'tongue' 'tongue'\n",
      " 'right_hand' 'left_hand' 'right_hand' 'feet' 'feet' 'feet' 'tongue'\n",
      " 'feet' 'left_hand' 'tongue' 'right_hand' 'feet' 'right_hand' 'feet'\n",
      " 'tongue' 'right_hand' 'feet' 'left_hand' 'left_hand' 'left_hand' 'tongue'\n",
      " 'right_hand' 'left_hand' 'feet' 'left_hand' 'feet' 'right_hand' 'tongue'\n",
      " 'left_hand' 'feet' 'feet' 'left_hand' 'feet' 'right_hand' 'tongue'\n",
      " 'tongue' 'tongue' 'feet' 'left_hand' 'tongue' 'right_hand' 'tongue'\n",
      " 'right_hand' 'left_hand' 'feet' 'right_hand' 'left_hand' 'feet' 'feet'\n",
      " 'left_hand' 'feet' 'tongue' 'tongue' 'right_hand' 'left_hand'\n",
      " 'right_hand' 'tongue' 'right_hand' 'tongue' 'feet' 'right_hand'\n",
      " 'right_hand' 'right_hand' 'feet' 'tongue' 'left_hand' 'right_hand'\n",
      " 'tongue' 'left_hand' 'feet' 'feet' 'tongue' 'left_hand' 'left_hand'\n",
      " 'feet' 'right_hand' 'tongue' 'tongue' 'tongue' 'right_hand' 'left_hand'\n",
      " 'feet' 'right_hand' 'tongue' 'left_hand' 'tongue' 'feet' 'right_hand'\n",
      " 'tongue' 'tongue' 'left_hand' 'right_hand' 'right_hand' 'feet' 'tongue'\n",
      " 'right_hand' 'left_hand' 'left_hand' 'tongue' 'right_hand' 'left_hand'\n",
      " 'feet' 'right_hand' 'right_hand' 'feet' 'left_hand' 'tongue' 'feet'\n",
      " 'feet' 'feet' 'feet' 'left_hand' 'right_hand' 'left_hand' 'right_hand'\n",
      " 'left_hand' 'left_hand' 'feet' 'feet' 'right_hand' 'feet' 'tongue'\n",
      " 'left_hand' 'tongue' 'left_hand' 'left_hand' 'right_hand' 'tongue' 'feet'\n",
      " 'right_hand' 'tongue' 'feet' 'tongue' 'feet' 'tongue' 'right_hand'\n",
      " 'right_hand' 'tongue' 'left_hand' 'right_hand' 'right_hand' 'right_hand'\n",
      " 'feet' 'tongue' 'left_hand' 'tongue' 'left_hand' 'feet' 'left_hand'\n",
      " 'tongue' 'left_hand' 'feet' 'left_hand' 'right_hand' 'feet' 'feet'\n",
      " 'tongue' 'left_hand' 'right_hand' 'tongue' 'right_hand' 'feet' 'feet'\n",
      " 'left_hand' 'tongue' 'right_hand' 'tongue' 'left_hand' 'left_hand' 'feet'\n",
      " 'feet' 'right_hand' 'tongue' 'right_hand' 'right_hand' 'left_hand'\n",
      " 'right_hand' 'tongue' 'tongue' 'right_hand' 'right_hand' 'right_hand'\n",
      " 'right_hand' 'tongue' 'tongue' 'feet' 'tongue' 'left_hand' 'right_hand'\n",
      " 'feet' 'right_hand' 'left_hand' 'tongue' 'left_hand' 'tongue' 'left_hand'\n",
      " 'left_hand' 'left_hand' 'left_hand' 'feet' 'feet' 'tongue' 'right_hand'\n",
      " 'feet' 'feet' 'feet' 'tongue' 'feet' 'left_hand' 'feet']\n",
      "\n",
      "\n",
      "labeles_length: \n",
      " 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eier\\miniconda3\\envs\\mne\\Lib\\site-packages\\moabb\\paradigms\\base.py:354: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  X = mne.concatenate_epochs(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'start = 0\\nraw_arrays = []\\nfor i in range(48):\\n    print(\"start:\", start)\\n    end = (i+1) * samplesInTrial\\n    print(\"end\", end)\\n    # Slice the data to ensure each slice has the same shape\\n    sliced_data = data[0][start:end].T\\n    raw_arrays.append(RawArray(sliced_data, info=info))\\n    start = end\\n\\nevent_labels = data[2]\\nprint(event_labels) '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#using moabb to make the epochs\n",
    "\n",
    "from moabb.datasets import BNCI2014_001\n",
    "from moabb.evaluations import CrossSessionEvaluation\n",
    "from moabb.paradigms import MotorImagery\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "from mne import EpochsArray\n",
    "\n",
    "#loading the dataset\n",
    "\n",
    "dataset = BNCI2014_001()\n",
    "dataset.subject_list = dataset.subject_list[:3] \n",
    "print(\"Subject_list_dataset: \\n\",dataset.subject_list)\n",
    "print(\"\\n\")\n",
    "datasets = [dataset]\n",
    "paradigm =MotorImagery()\n",
    "\n",
    "#Get the data as MNE epochs\n",
    "subject_list = [1]\n",
    "sessions = dataset.get_data(subject_list)\n",
    "print(\"Sessions: \\n\")\n",
    "for i in sessions:\n",
    "    print(i)\n",
    "    print(\"\\n\")\n",
    "print(\"Sessions: \\n\",sessions)\n",
    "print(\"\\n\")\n",
    "\n",
    "'''#this returns numpy array\n",
    "X, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)'''\n",
    "\n",
    "#This returns epochs as its set to true\n",
    "epochs, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset, subjects=subject_list, return_epochs=True\n",
    ")\n",
    "print(\"epochs: \\n\", epochs)\n",
    "print(\"\\n\")\n",
    "print(\"labels: \\n\", labels)\n",
    "print(\"\\n\")\n",
    "print(\"labeles_length: \\n\", len(labels))\n",
    "\n",
    "\n",
    "'''start = 0\n",
    "raw_arrays = []\n",
    "for i in range(48):\n",
    "    print(\"start:\", start)\n",
    "    end = (i+1) * samplesInTrial\n",
    "    print(\"end\", end)\n",
    "    # Slice the data to ensure each slice has the same shape\n",
    "    sliced_data = data[0][start:end].T\n",
    "    raw_arrays.append(RawArray(sliced_data, info=info))\n",
    "    start = end\n",
    "\n",
    "event_labels = data[2]\n",
    "print(event_labels) '''\n",
    "#48 labels for the 48 trials in this first session\n",
    "\n",
    "#epochs_arrays = EpochsArray(raw_arrays, info=info)\n",
    "#epochs_arrays.plot(picks=\"misc\", show_scrollbars=False, events=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = 0\\nraw_arrays = []\\nfor i in range(48):\\n    print(\"start:\", start)\\n    end = (i+1) * samplesInTrial\\n    print(\"end\", end)\\n    # Slice the data to ensure each slice has the same shape\\n    sliced_data = data[0][start:end].T\\n    raw_arrays.append(RawArray(sliced_data, info=info))\\n    start = end\\n\\nevent_labels = data[2]\\nprint(event_labels) '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Epochs\n",
    "import scipy.io as sio\n",
    "from moabb.datasets import BNCI2014_001\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "from mne import EpochsArray\n",
    "\n",
    "# Load the mat file of the first subject's training data\n",
    "file = sio.loadmat('A01T.mat')\n",
    "\n",
    "# Extract EEG data for the first\n",
    "data = file['data'][0]\n",
    "# Remove the first 3 EOG runs of each session\n",
    "data = data[3:]\n",
    "data = data[0][0][0]\n",
    "\n",
    "# Extract necessary information\n",
    "num_channels = 25  # Number of EEG channels\n",
    "sampling_freq = 250  # Sampling frequency\n",
    "\n",
    "# Create MNE info instance\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\n",
    "\n",
    "print(data[0].T)\n",
    "print((data[0]).T.shape)\n",
    "print(data[0][0][0].T)\n",
    "timeInTrial = (data[0].shape[0] / 250.0) /48\n",
    "samplesInTrial = data[0].shape[0] // 48\n",
    "print(\"samplesintrial:\", samplesInTrial)\n",
    "# Assuming 250 Hz sampling frequency\n",
    "\n",
    "# Create RawArray objects and plot the data\n",
    "\n",
    "##best way - find the event type markers - 769, 770,771,772 to identify exact cue onsets - were are they tho?, \n",
    "#then add 4 seconds(4 seconds are 1000 samples, 4*250Hz) to that time/sample to get t= 2 to t =6\n",
    "\n",
    "##array for each trial and the concatenate? also need to add montage - electrode positions?\n",
    "\n",
    "raw_array_1T = RawArray(data[0].T, info=info)\n",
    "\n",
    "dataset = BNCI2014_001\n",
    "\n",
    "subject_list = [1]\n",
    "sessions = dataset.get_data(subject_list)\n",
    "X, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)\n",
    "epochs, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset, subjects=subject_list, return_epochs=True\n",
    "'''\n",
    "'''start = 0\n",
    "raw_arrays = []\n",
    "for i in range(48):\n",
    "    print(\"start:\", start)\n",
    "    end = (i+1) * samplesInTrial\n",
    "    print(\"end\", end)\n",
    "    # Slice the data to ensure each slice has the same shape\n",
    "    sliced_data = data[0][start:end].T\n",
    "    raw_arrays.append(RawArray(sliced_data, info=info))\n",
    "    start = end\n",
    "\n",
    "event_labels = data[2]\n",
    "print(event_labels) '''\n",
    "#48 labels for the 48 trials in this first session\n",
    "\n",
    "#epochs_arrays = EpochsArray(raw_arrays, info=info)\n",
    "#epochs_arrays.plot(picks=\"misc\", show_scrollbars=False, events=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Epochs\\nimport scipy.io as sio\\nfrom moabb.datasets import BNCI2014_001\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport mne\\nfrom mne.io import RawArray\\nfrom mne import EpochsArray\\n\\n# Load the mat file of the first subject\\'s training data\\nfile = sio.loadmat(\\'A01T.mat\\')\\n\\n# Extract EEG data for the first\\ndata = file[\\'data\\'][0]\\n# Remove the first 3 EOG runs of each session\\ndata = data[3:]\\ndata = data[0][0][0]\\n\\n# Extract necessary information\\nnum_channels = 25  # Number of EEG channels\\nsampling_freq = 250  # Sampling frequency\\n\\n# Create MNE info instance\\ninfo = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=[\\'eeg\\']*22 + [\\'eog\\']*3)\\n\\nprint(data[0].T)\\nprint((data[0]).T.shape)\\nprint(data[0][0][0].T)\\ntimeInTrial = (data[0].shape[0] / 250.0) /48\\nsamplesInTrial = data[0].shape[0] // 48\\nprint(\"samplesintrial:\", samplesInTrial)\\n# Assuming 250 Hz sampling frequency\\n\\n# Create RawArray objects and plot the data\\n\\n##best way - find the event type markers - 769, 770,771,772 to identify exact cue onsets - were are they tho?, \\n#then add 4 seconds(4 seconds are 1000 samples, 4*250Hz) to that time/sample to get t= 2 to t =6\\n\\n##array for each trial and the concatenate? also need to add montage - electrode positions?\\n\\nraw_array_1T = RawArray(data[0].T, info=info)\\n\\ndataset = BNCI2014_001\\n\\nsubject_list = [1]\\nsessions = dataset.get_data(subject_list)\\nX, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)\\nepochs, labels, meta = paradigm.get_data(\\n    dataset=dataset, subjects=subject_list, return_epochs=True\\n\\nstart = 0\\nraw_arrays = []\\nfor i in range(48):\\n    print(\"start:\", start)\\n    end = (i+1) * samplesInTrial\\n    print(\"end\", end)\\n    # Slice the data to ensure each slice has the same shape\\n    sliced_data = data[0][start:end].T\\n    raw_arrays.append(RawArray(sliced_data, info=info))\\n    start = end\\n\\nevent_labels = data[2]\\nprint(event_labels) '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Epochs\n",
    "import scipy.io as sio\n",
    "from moabb.datasets import BNCI2014_001\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "from mne import EpochsArray\n",
    "\n",
    "# Load the mat file of the first subject's training data\n",
    "file = sio.loadmat('A01T.mat')\n",
    "\n",
    "# Extract EEG data for the first\n",
    "data = file['data'][0]\n",
    "# Remove the first 3 EOG runs of each session\n",
    "data = data[3:]\n",
    "data = data[0][0][0]\n",
    "\n",
    "# Extract necessary information\n",
    "num_channels = 25  # Number of EEG channels\n",
    "sampling_freq = 250  # Sampling frequency\n",
    "\n",
    "# Create MNE info instance\n",
    "info = mne.create_info(num_channels, sfreq=sampling_freq, ch_types=['eeg']*22 + ['eog']*3)\n",
    "\n",
    "print(data[0].T)\n",
    "print((data[0]).T.shape)\n",
    "print(data[0][0][0].T)\n",
    "timeInTrial = (data[0].shape[0] / 250.0) /48\n",
    "samplesInTrial = data[0].shape[0] // 48\n",
    "print(\"samplesintrial:\", samplesInTrial)\n",
    "# Assuming 250 Hz sampling frequency\n",
    "\n",
    "# Create RawArray objects and plot the data\n",
    "\n",
    "##best way - find the event type markers - 769, 770,771,772 to identify exact cue onsets - were are they tho?, \n",
    "#then add 4 seconds(4 seconds are 1000 samples, 4*250Hz) to that time/sample to get t= 2 to t =6\n",
    "\n",
    "##array for each trial and the concatenate? also need to add montage - electrode positions?\n",
    "\n",
    "raw_array_1T = RawArray(data[0].T, info=info)\n",
    "\n",
    "dataset = BNCI2014_001\n",
    "\n",
    "subject_list = [1]\n",
    "sessions = dataset.get_data(subject_list)\n",
    "X, labels, meta = paradigm.get_data(dataset=dataset, subjects=subject_list)\n",
    "epochs, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset, subjects=subject_list, return_epochs=True\n",
    "\n",
    "start = 0\n",
    "raw_arrays = []\n",
    "for i in range(48):\n",
    "    print(\"start:\", start)\n",
    "    end = (i+1) * samplesInTrial\n",
    "    print(\"end\", end)\n",
    "    # Slice the data to ensure each slice has the same shape\n",
    "    sliced_data = data[0][start:end].T\n",
    "    raw_arrays.append(RawArray(sliced_data, info=info))\n",
    "    start = end\n",
    "\n",
    "event_labels = data[2]\n",
    "print(event_labels) '''\n",
    "#48 labels for the 48 trials in this first session\n",
    "\n",
    "#epochs_arrays = EpochsArray(raw_arrays, info=info)\n",
    "#epochs_arrays.plot(picks=\"misc\", show_scrollbars=False, events=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing from all possible events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "576 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eier\\miniconda3\\envs\\mne\\Lib\\site-packages\\moabb\\paradigms\\base.py:354: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  X = mne.concatenate_epochs(X)\n"
     ]
    }
   ],
   "source": [
    "from moabb.analysis.plotting import paired_plot, summary_plot\n",
    "from moabb.datasets import BNCI2014_001\n",
    "from moabb.evaluations import CrossSessionEvaluation\n",
    "from moabb.paradigms import MotorImagery\n",
    "\n",
    "dataset = BNCI2014_001()\n",
    "#dataset.subject_list = dataset.subject_list[:3]\n",
    "datasets = [dataset]\n",
    "paradigm = MotorImagery()\n",
    "\n",
    "subject_list = [1]\n",
    "sessions = dataset.get_data(subject_list)\n",
    "#returns mne epochs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "576 matching events found\n",
      "No baseline correction applied\n",
      "{'0': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '1': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '2': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '3': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '4': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '5': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>}\n",
      "\n",
      "\n",
      "{'0': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '1': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '2': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '3': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '4': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>, '5': <RawArray | 26 x 96735 (386.9 s), ~19.2 MB, data loaded>}\n",
      "\n",
      "\n",
      "[[    250       0       4]\n",
      " [   2253       0       3]\n",
      " [   4171       0       2]\n",
      " ...\n",
      " [1176911       0       3]\n",
      " [1179025       0       1]\n",
      " [1181084       0       3]]\n",
      "{'left_hand': 1, 'right_hand': 2, 'feet': 3, 'tongue': 4}\n",
      "2.0 6.0\n",
      "<EpochsArray |  576 events (all good), 2 – 6 s, baseline off, ~96.8 MB, data loaded, with metadata,\n",
      " 'left_hand': 144\n",
      " 'right_hand': 144\n",
      " 'feet': 144\n",
      " 'tongue': 144>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eier\\miniconda3\\envs\\mne\\Lib\\site-packages\\moabb\\paradigms\\base.py:354: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  X = mne.concatenate_epochs(X)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset, subjects=subject_list, return_epochs=True\n",
    ")\n",
    "\n",
    "print(sessions[1][\"0train\"])\n",
    "print(\"\\n\")\n",
    "print(sessions[1][\"1test\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(epochs.events)\n",
    "print(epochs.event_id)\n",
    "print(epochs.tmin, epochs.tmax)\n",
    "print(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events  [[    250       0       4]\n",
      " [   2253       0       3]\n",
      " [   4171       0       2]\n",
      " ...\n",
      " [1176911       0       3]\n",
      " [1179025       0       1]\n",
      " [1181084       0       3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"events \", epochs.events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  [[    250       0       4]\n",
      " [   2253       0       3]\n",
      " [   4171       0       2]\n",
      " ...\n",
      " [1176911       0       3]\n",
      " [1179025       0       1]\n",
      " [1181084       0       3]]\n",
      "lables after:  [ 0 -1 -2 -3 -3 -2 -1  0 -2 -1 -3 -3 -3  0 -2 -2 -3 -3 -1 -3 -2  0  0 -1\n",
      " -3  0  0 -2  0  0 -2 -3 -2 -1 -1 -1  0 -1 -3  0 -2 -1 -2 -1  0 -2 -1 -3\n",
      " -3 -3  0 -2 -3 -1 -3 -1 -2  0 -3 -1 -1 -3 -1 -2  0  0  0 -1 -3  0 -2  0\n",
      " -2 -3 -1 -2 -3 -1 -1 -3 -1  0  0 -2 -3 -2  0 -2  0 -1 -2 -2 -2 -1  0 -3\n",
      " -2  0 -3 -1 -1  0 -3 -3 -1 -2  0  0  0 -2 -3 -1 -2  0 -3  0 -1 -2  0  0\n",
      " -3 -2 -2 -1  0 -2 -3 -3  0 -2 -3 -1 -2 -2 -1 -3  0 -1 -1 -1 -1 -3 -2 -3\n",
      " -2 -3 -3 -1 -1 -2 -1  0 -3  0 -3 -3 -2  0 -1 -2  0 -1  0 -1  0 -2 -2  0\n",
      " -3 -2 -2 -2 -1  0 -3  0 -3 -1 -3  0 -3 -1 -3 -2 -1 -1  0 -3 -2  0 -2 -1\n",
      " -1 -3  0 -2  0 -3 -3 -1 -1 -2  0 -2 -2 -3 -2  0  0 -2 -2 -2 -2  0  0 -1\n",
      "  0 -3 -2 -1 -2 -3  0 -3  0 -3 -3 -3 -3 -1 -1  0 -2 -1 -1 -1  0 -1 -3 -1\n",
      " -3  0 -2  0  0 -1  0  0 -1 -2  0 -1 -3 -2 -2 -2 -1 -2  0 -1  0 -2 -1 -3\n",
      "  0 -3 -1  0 -3 -1 -3 -2 -2 -3  0 -3  0 -1 -1 -3 -1 -2 -2 -3 -1 -3 -2 -3\n",
      " -3 -2 -2 -3 -2 -3 -2 -1 -2  0 -3 -1 -2 -3  0  0  0  0  0 -3 -1 -2 -3 -3\n",
      " -1  0 -3 -1 -1 -1 -3 -2 -3 -2 -2 -3 -2 -1 -2 -1 -1  0 -1 -1  0  0  0  0\n",
      "  0 -1 -2 -3 -3 -2 -1  0 -2 -1 -3 -3 -3  0 -2 -2 -3 -3 -1 -3 -2  0  0 -1\n",
      " -3  0  0 -2  0  0 -2 -3 -2 -1 -1 -1  0 -1 -3  0 -2 -1 -2 -1  0 -2 -1 -3\n",
      " -3 -3  0 -2 -3 -1 -3 -1 -2  0 -3 -1 -1 -3 -1 -2  0  0  0 -1 -3  0 -2  0\n",
      " -2 -3 -1 -2 -3 -1 -1 -3 -1  0  0 -2 -3 -2  0 -2  0 -1 -2 -2 -2 -1  0 -3\n",
      " -2  0 -3 -1 -1  0 -3 -3 -1 -2  0  0  0 -2 -3 -1 -2  0 -3  0 -1 -2  0  0\n",
      " -3 -2 -2 -1  0 -2 -3 -3  0 -2 -3 -1 -2 -2 -1 -3  0 -1 -1 -1 -1 -3 -2 -3\n",
      " -2 -3 -3 -1 -1 -2 -1  0 -3  0 -3 -3 -2  0 -1 -2  0 -1  0 -1  0 -2 -2  0\n",
      " -3 -2 -2 -2 -1  0 -3  0 -3 -1 -3  0 -3 -1 -3 -2 -1 -1  0 -3 -2  0 -2 -1\n",
      " -1 -3  0 -2  0 -3 -3 -1 -1 -2  0 -2 -2 -3 -2  0  0 -2 -2 -2 -2  0  0 -1\n",
      "  0 -3 -2 -1 -2 -3  0 -3  0 -3 -3 -3 -3 -1 -1  0 -2 -1 -1 -1  0 -1 -3 -1]\n",
      "[0.25 0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"before: \", epochs.events)\n",
    "labels = epochs.events[:, -1] -4\n",
    "print(\"lables after: \", labels)\n",
    "\n",
    "class_balance = np.zeros(4)\n",
    "for i in range(4):\n",
    "    class_balance[i] = np.mean(labels == i)\n",
    "print(class_balance)\n",
    "class_balance = np.max(class_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.52359238e-06,  6.05479173e-06,  5.55222300e-06, ...,\n",
       "         -5.72856071e-07, -2.82827100e-06, -5.00875118e-06],\n",
       "        [ 1.91658213e-06,  1.74039129e-06,  8.80343196e-07, ...,\n",
       "          5.06184938e-07, -3.31466548e-06, -6.65938125e-06],\n",
       "        [ 3.41951163e-06,  3.84870584e-06,  3.50987378e-06, ...,\n",
       "         -1.52997242e-06, -5.09625352e-06, -7.88780877e-06],\n",
       "        ...,\n",
       "        [-1.18565960e-06, -1.51034610e-06, -2.41203985e-06, ...,\n",
       "         -2.33472371e-06, -6.69460626e-06, -9.35521896e-06],\n",
       "        [-2.35736743e-06, -3.05259983e-06, -3.87318513e-06, ...,\n",
       "         -2.61970359e-06, -6.21065549e-06, -8.26673539e-06],\n",
       "        [-1.09144736e-06, -1.01641231e-06, -1.73219282e-06, ...,\n",
       "          6.21774064e-08, -3.33972413e-06, -5.84536201e-06]],\n",
       "\n",
       "       [[-5.94328262e-06, -5.49236729e-06, -3.71652245e-06, ...,\n",
       "          9.71111097e-06,  8.34966036e-06,  5.93386064e-06],\n",
       "        [-6.39517030e-06, -6.41812583e-06, -4.88865165e-06, ...,\n",
       "          7.27015452e-06,  6.35452504e-06,  4.52285191e-06],\n",
       "        [-6.96987223e-06, -7.41402605e-06, -6.09594180e-06, ...,\n",
       "          8.97296514e-06,  8.70369885e-06,  6.93789219e-06],\n",
       "        ...,\n",
       "        [-3.29319561e-06, -4.67209558e-06, -5.52384311e-06, ...,\n",
       "          9.57450838e-06,  7.74003407e-06,  4.74884605e-06],\n",
       "        [-1.22464151e-06, -2.19039382e-06, -3.14118476e-06, ...,\n",
       "          1.08509091e-05,  9.50479102e-06,  6.74403318e-06],\n",
       "        [-8.64881378e-08, -1.53347076e-06, -3.16685712e-06, ...,\n",
       "          9.95459030e-06,  7.80445086e-06,  4.96133712e-06]],\n",
       "\n",
       "       [[ 6.12675343e-06,  4.80069492e-06,  1.89375930e-06, ...,\n",
       "         -1.80963356e-06, -9.58220052e-07,  1.00182141e-07],\n",
       "        [ 5.58056783e-06,  3.78074325e-06,  1.02786715e-06, ...,\n",
       "         -1.32946883e-06, -6.22932545e-07,  6.76845058e-08],\n",
       "        [ 7.43279059e-06,  5.46171881e-06,  2.11057866e-06, ...,\n",
       "         -5.75760395e-07, -6.53766659e-07, -7.35656782e-07],\n",
       "        ...,\n",
       "        [ 7.46558200e-06,  3.39468294e-06, -1.15504664e-06, ...,\n",
       "         -3.41820391e-06, -2.94795975e-06, -2.31999583e-06],\n",
       "        [ 5.00811146e-06,  1.58727604e-06, -2.11582106e-06, ...,\n",
       "         -3.48512891e-06, -2.69524861e-06, -1.68924441e-06],\n",
       "        [ 8.01582351e-06,  4.22747790e-06, -1.41063203e-07, ...,\n",
       "         -4.09716300e-06, -3.54158428e-06, -2.65380789e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.64914939e-06,  4.71489792e-06,  4.30249646e-06, ...,\n",
       "         -4.46432994e-06, -8.80468102e-07,  1.83638242e-06],\n",
       "        [ 7.83455173e-06,  5.90839650e-06,  4.12150267e-06, ...,\n",
       "         -2.75260905e-06, -3.31370108e-10,  1.86447411e-06],\n",
       "        [ 6.62818496e-06,  4.96602288e-06,  3.86715610e-06, ...,\n",
       "         -2.28117673e-06,  6.68347734e-07,  2.38807903e-06],\n",
       "        ...,\n",
       "        [ 5.38295964e-07, -1.25268645e-06, -1.97120103e-06, ...,\n",
       "         -3.43161487e-06, -1.84636281e-06, -1.05408916e-06],\n",
       "        [ 1.74914650e-06,  1.08032346e-06,  1.06539763e-06, ...,\n",
       "         -4.79101574e-06, -3.03659297e-06, -1.80735476e-06],\n",
       "        [ 2.28629442e-06,  1.25303632e-06,  1.11157945e-06, ...,\n",
       "         -4.54980916e-06, -3.11270548e-06, -2.35874041e-06]],\n",
       "\n",
       "       [[ 7.05169281e-07, -1.96962180e-06, -3.28150943e-06, ...,\n",
       "         -3.07774664e-06, -2.15279118e-06, -7.20322355e-07],\n",
       "        [-1.12351555e-06, -2.83756174e-06, -2.78149070e-06, ...,\n",
       "         -3.06436595e-06, -2.52954674e-06, -1.72025114e-06],\n",
       "        [ 3.00874882e-07, -2.44722873e-06, -3.67788679e-06, ...,\n",
       "         -5.00651651e-06, -3.64932910e-06, -1.50497277e-06],\n",
       "        ...,\n",
       "        [-2.01612753e-06, -2.44827552e-06, -1.68194009e-06, ...,\n",
       "         -8.70340222e-06, -8.03403831e-06, -5.99541708e-06],\n",
       "        [-2.65958251e-06, -3.76009414e-06, -3.63224902e-06, ...,\n",
       "         -7.78353027e-06, -7.15988622e-06, -5.37919690e-06],\n",
       "        [-3.75229019e-06, -2.98075739e-06, -9.32246646e-07, ...,\n",
       "         -7.41236337e-06, -7.49863884e-06, -6.62258743e-06]],\n",
       "\n",
       "       [[-7.92003065e-08,  2.86419133e-06,  6.20442263e-06, ...,\n",
       "          3.85826380e-07, -4.88332462e-06, -8.92099296e-06],\n",
       "        [-2.41516072e-06, -2.28966771e-07,  3.08509918e-06, ...,\n",
       "          1.91739797e-06, -2.82707031e-06, -6.51885735e-06],\n",
       "        [-1.61647899e-06,  1.32887769e-06,  5.14853164e-06, ...,\n",
       "          2.34305266e-06, -2.32146193e-06, -6.16170314e-06],\n",
       "        ...,\n",
       "        [-1.47900495e-06,  2.54296264e-06,  6.01880066e-06, ...,\n",
       "         -5.41888056e-07, -2.87932016e-06, -3.97632416e-06],\n",
       "        [-2.60973852e-06,  1.31636226e-06,  4.76973318e-06, ...,\n",
       "         -1.39559072e-06, -4.06517505e-06, -5.20911883e-06],\n",
       "        [-9.14311817e-07,  2.99021251e-06,  6.11174127e-06, ...,\n",
       "         -2.50169597e-06, -3.02738608e-06, -2.23344428e-06]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_data =epochs.get_data(copy=False)\n",
    "epochs_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
