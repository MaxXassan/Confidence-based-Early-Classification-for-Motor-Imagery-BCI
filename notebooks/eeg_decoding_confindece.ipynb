{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:241: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/pipelines/__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n",
      "Choosing from all possible events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the get_shape_from_baseconcar, InputShapeSetterEEG, BraindecodeDatasetLoaderyou need to install `braindecode`.`pip install braindecode` or Please refer to `https://braindecode.org`.\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "Adding metadata with 3 columns\n",
      "576 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/paradigms/base.py:354: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  X = mne.concatenate_epochs(X)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from mne.decoding import CSP\n",
    "\n",
    "current_directory = os.path.abspath('')\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Early_predict_UQ.data.make_dataset import make_data\n",
    "\n",
    "def plot_over_time(w_times, class_balance, onset, scores_windows = None, confidence_windows = None):\n",
    "    plt.figure()\n",
    "    if (scores_windows != None):\n",
    "        plt.plot(w_times, np.mean(scores_windows, 0), label=\"Score\")\n",
    "        plt.axvline(onset, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "        plt.axhline(class_balance, linestyle=\"-\", color=\"k\", label=\"Chance\")\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"classification accuracy\")\n",
    "        plt.title(\"Classification score over time\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    if (confidence_windows != None):\n",
    "        plt.plot(w_times, np.mean(confidence_windows, 0), label=\"Score\")\n",
    "        plt.axvline(onset, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "        plt.axhline(0.5, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"confidence\")\n",
    "        plt.title(\"Classification score over time\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "subject_list = [1]\n",
    "# Preprocessed epochs\n",
    "epochs, labels = make_data(subject_list)\n",
    "#epochs.plot()\n",
    "#print(\"epochs shape: \",epochs.shape)\n",
    "#print(\"labels shape: \",labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probabilities):\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    return entropy\n",
    "def find_mean_probabilities(probabilities):\n",
    "   return np.mean(probabilities, axis=0)\n",
    "\n",
    "# Calculate maximum probability\n",
    "def calculate_max_mean_probability_and_class(probabilities):\n",
    "    max_mean_probability_index = np.argmax(np.mean(probabilities, axis=0))\n",
    "    max_mean_probability = np.max(np.mean(probabilities, axis=0))\n",
    "    corresponding_class = max_mean_probability_index + 1  # Classes are typically indexed starting from 1\n",
    "    return max_mean_probability, corresponding_class\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    " \n",
    "    return num\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "# Asserting the epochs and labels (last row of the events matrix) to be used for the classification\n",
    "epochs_train = epochs.copy()\n",
    "labels = epochs.events[:, -1] - 4\n",
    "\n",
    "# Cross validation \n",
    "## (Might need to do cross session - session 1 as train, and session 2 as test. See dataset_structure.ipynb)\n",
    "scores = []\n",
    "epochs_data = epochs.get_data(copy=False)\n",
    "print(\"len of epochs data:\\n\",len(epochs_data))\n",
    "epochs_data_train = epochs_train.get_data(copy=False)\n",
    "cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "cv_split = cv.split(epochs_data_train)\n",
    "print(\"cv split: \\n\", cv_split)\n",
    "\n",
    "# LDA and CSP pipeline\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "\n",
    "# Class balance between the 4 classes. \n",
    "class_balance = np.zeros(4)\n",
    "for i in range(4):\n",
    "    class_balance[i] = np.mean(labels == i)\n",
    "\n",
    "class_balance = np.max(class_balance)\n",
    "\n",
    "sfreq = 250 # Sampling frequency of 250 Hz as per the BCI competion dataset 2a\n",
    "\n",
    "# Classify the signal using a sliding window\n",
    "\n",
    "w_length = int(sfreq * 0.5)  # Window length\n",
    "w_step = int(sfreq * 0.1)  # window step size\n",
    "w_start = np.arange(0, epochs_data.shape[2] - w_length, w_step) # Set of starting positions in the signal(Note! the signal is 2s to 4s) \n",
    "#if len=0.5 and step = 0.1. Theres 36 starting points 0.1 s apart, multiplied by the sfreq\n",
    "print(\"w start shape: \", w_start.shape)\n",
    "print(\"w start: \\n\", w_start)\n",
    "scores_windows = []  \n",
    "\n",
    "#threshold = 0.5\n",
    "confidence_windows = []\n",
    "counter = 0\n",
    "# Running classification across the signal\n",
    "for train_idx, test_idx in cv_split:\n",
    "    print(\"train idx: \", train_idx)\n",
    "    print(\"test idx: \", test_idx)\n",
    "    print(\"nr train_index:\", len(train_idx))\n",
    "    print(\"nr test_index:\", len(test_idx))\n",
    "    counter +=1\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx] # Get the current labels and data\n",
    "    # Exatract spatial filters and transform the data as a whole\n",
    "    X_train = csp.fit_transform(epochs_data_train[train_idx], y_train)\n",
    "    X_test = csp.transform(epochs_data_train[test_idx]) #  why define it here!\n",
    "\n",
    "    # Fit the classifier on the training data\n",
    "    lda.fit(X_train, y_train)\n",
    "    w_times = (w_start + w_length / 2.0) / sfreq + epochs.tmin\n",
    "\n",
    "\n",
    "    #FIX THE STUFF LOL, IM TOO TIRED TO MAKE A PLAN 4 U BABES <33 LOVE U\n",
    "    # Test the classifier on the windows. This is where we run over the signal\n",
    "    score_this_window = []\n",
    "    confidence_this_window = []\n",
    "    max_classes = []\n",
    "    num = 1\n",
    "    for n in w_start:\n",
    "        X_test = csp.transform(epochs_data[test_idx][:, :, n : (n + w_length)])\n",
    "        score_this_window.append(lda.score(X_test, y_test))\n",
    "        probabilities = lda.predict_proba(X_test)\n",
    "        print(\"prob shape: \", probabilities.shape)\n",
    "        #print(\"probabilities: \\n\", probabilities)\n",
    "\n",
    "        #print(\"prob means of the classes: \\n \", find_mean_probabilities(probabilities))\n",
    "        mean_probs = find_mean_probabilities(probabilities)\n",
    "\n",
    "        #entropy = calculate_entropy(probabilities)\n",
    "       \n",
    "        confidence_this_window.append(mean_probs)\n",
    "\n",
    "    #most_freq_max_class = most_frequent(max_classes)\n",
    "    \n",
    "    # Plot confidence \n",
    "    plt.plot(w_times, confidence_this_window, label = [\"1\",\"2\",\"3\",\"4\"])\n",
    "    plt.plot\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Confidence\")\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.axhline(0.5, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "    plt.title(\"Classification Confidence over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    scores_windows.append(score_this_window)\n",
    "    #print(\"right label:\", y_test[?])\n",
    "    if counter == 2:\n",
    "        break\n",
    "\n",
    "    '''plt.plot(w_times,mean_confidence_class_1, label=\"Confidence Class 1\")\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.axhline(0.5, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.ylabel(\"confidence\")\n",
    "    plt.title(\"Classification confidence over time\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()'''\n",
    "   \n",
    "    \n",
    "# Plot the scores over time\n",
    "#plot_over_time(w_times, class_balance, epochs.tmin, scores_windows, confidence_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Slding with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing from all possible events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /workspaces/UQ_Early_prediction_MI_BCI\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m subject_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Choosing the subject or subjects\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Preprocessed epochs\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m epochs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mmake_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Asserting the epochs and labels (last row of the events matrix) to be used for the classification\u001b[39;00m\n\u001b[1;32m     48\u001b[0m epochs_train \u001b[38;5;241m=\u001b[39m epochs\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/Early_predict_UQ/data/make_dataset.py:21\u001b[0m, in \u001b[0;36mmake_data\u001b[0;34m(subject_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m dataset \u001b[38;5;241m=\u001b[39m BNCI2014_001()\n\u001b[1;32m     20\u001b[0m paradigm \u001b[38;5;241m=\u001b[39m MotorImagery(fmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, fmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;66;03m# Bandpass filter between to enhance mu and beta frequencies\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m epochs, labels, _ \u001b[38;5;241m=\u001b[39m \u001b[43mparadigm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epochs, labels\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/paradigms/base.py:278\u001b[0m, in \u001b[0;36mBaseProcessing.get_data\u001b[0;34m(self, dataset, subjects, return_epochs, return_raws, cache_config, postprocess_pipeline)\u001b[0m\n\u001b[1;32m    273\u001b[0m process_pipelines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_process_pipelines(\n\u001b[1;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    276\u001b[0m labels_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[0;32m--> 278\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_pipelines\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    287\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    288\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/paradigms/base.py:279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    273\u001b[0m process_pipelines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_process_pipelines(\n\u001b[1;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    276\u001b[0m labels_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[1;32m    278\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m process_pipeline \u001b[38;5;129;01min\u001b[39;00m process_pipelines\n\u001b[1;32m    285\u001b[0m ]\n\u001b[1;32m    287\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    288\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/datasets/base.py:342\u001b[0m, in \u001b[0;36mBaseDataset.get_data\u001b[0;34m(self, subjects, cache_config, process_pipeline)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subject \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubject_list:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid subject \u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(subject))\n\u001b[0;32m--> 342\u001b[0m     data[subject] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_single_subject_data_using_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m check_subject_names(data)\n\u001b[1;32m    348\u001b[0m check_session_names(data)\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/datasets/base.py:466\u001b[0m, in \u001b[0;36mBaseDataset._get_single_subject_data_using_cache\u001b[0;34m(self, subject, cache_config, process_pipeline)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Apply remaining steps and save:\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_idx, (step_type, process_pipeline) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(remaining_steps):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# apply one step:\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     sessions_data \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msessions_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# save:\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    476\u001b[0m         (\n\u001b[1;32m    477\u001b[0m             cache_config\u001b[38;5;241m.\u001b[39msave_raw\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (cache_config\u001b[38;5;241m.\u001b[39msave_array \u001b[38;5;129;01mand\u001b[39;00m step_type \u001b[38;5;129;01mis\u001b[39;00m StepType\u001b[38;5;241m.\u001b[39mARRAY)\n\u001b[1;32m    486\u001b[0m     ):\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/datasets/base.py:467\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Apply remaining steps and save:\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_idx, (step_type, process_pipeline) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(remaining_steps):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# apply one step:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     sessions_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 467\u001b[0m         session: \u001b[43m{\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m session, runs \u001b[38;5;129;01min\u001b[39;00m sessions_data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    472\u001b[0m     }\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# save:\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    476\u001b[0m         (\n\u001b[1;32m    477\u001b[0m             cache_config\u001b[38;5;241m.\u001b[39msave_raw\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (cache_config\u001b[38;5;241m.\u001b[39msave_array \u001b[38;5;129;01mand\u001b[39;00m step_type \u001b[38;5;129;01mis\u001b[39;00m StepType\u001b[38;5;241m.\u001b[39mARRAY)\n\u001b[1;32m    486\u001b[0m     ):\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/datasets/base.py:468\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Apply remaining steps and save:\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step_idx, (step_type, process_pipeline) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(remaining_steps):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# apply one step:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     sessions_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    467\u001b[0m         session: {\n\u001b[0;32m--> 468\u001b[0m             run: \u001b[43mapply_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m run, raw \u001b[38;5;129;01min\u001b[39;00m runs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    470\u001b[0m         }\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m session, runs \u001b[38;5;129;01min\u001b[39;00m sessions_data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    472\u001b[0m     }\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# save:\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    476\u001b[0m         (\n\u001b[1;32m    477\u001b[0m             cache_config\u001b[38;5;241m.\u001b[39msave_raw\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (cache_config\u001b[38;5;241m.\u001b[39msave_array \u001b[38;5;129;01mand\u001b[39;00m step_type \u001b[38;5;129;01mis\u001b[39;00m StepType\u001b[38;5;241m.\u001b[39mARRAY)\n\u001b[1;32m    486\u001b[0m     ):\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/moabb/datasets/base.py:101\u001b[0m, in \u001b[0;36mapply_step\u001b[0;34m(pipeline, obj)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# no events received by RawToEpochs:\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo events found\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_function_transformer.py:267\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 267\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m output_config \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# check the consistency between the column provided by `transform` and\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# the the column names provided by `get_feature_names_out`.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_function_transformer.py:394\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[0;34m(self, X, func, kw_args)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/mne/io/base.py:1128\u001b[0m, in \u001b[0;36mBaseRaw.filter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;129m@copy_doc\u001b[39m(FilterMixin\u001b[38;5;241m.\u001b[39mfilter)\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1127\u001b[0m ):  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_by_annotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_by_annotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-83>:10\u001b[0m, in \u001b[0;36mfilter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/mne/filter.py:2610\u001b[0m, in \u001b[0;36mFilterMixin.filter\u001b[0;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m si, (start, stop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(onsets, ends)):\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;66;03m# Only output filter params once (for info level), and only warn\u001b[39;00m\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# once about the length criterion (longest segment is too short)\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     use_verbose \u001b[38;5;241m=\u001b[39m verbose \u001b[38;5;28;01mif\u001b[39;00m si \u001b[38;5;241m==\u001b[39m max_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2610\u001b[0m     \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msfreq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;66;03m# update info if filter is applied to all data channels,\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;66;03m# and it's not a band-stop filter\u001b[39;00m\n\u001b[1;32m   2631\u001b[0m _filt_update_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo, update_info, l_freq, h_freq)\n",
      "File \u001b[0;32m<decorator-gen-78>:10\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/mne/filter.py:1114\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     data \u001b[38;5;241m=\u001b[39m _overlap_add_filter(data, filt, \u001b[38;5;28;01mNone\u001b[39;00m, phase, picks, n_jobs, copy, pad)\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1114\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_iir_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/mne/filter.py:651\u001b[0m, in \u001b[0;36m_iir_filter\u001b[0;34m(x, iir_params, picks, n_jobs, copy, phase)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m picks:\n\u001b[0;32m--> 651\u001b[0m         x[p] \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    653\u001b[0m     data_new \u001b[38;5;241m=\u001b[39m parallel(p_fun(x\u001b[38;5;241m=\u001b[39mx[p]) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m picks)\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4461\u001b[0m, in \u001b[0;36msosfiltfilt\u001b[0;34m(sos, x, axis, padtype, padlen)\u001b[0m\n\u001b[1;32m   4459\u001b[0m zi\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m [n_sections] \u001b[38;5;241m+\u001b[39m zi_shape\n\u001b[1;32m   4460\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m axis_slice(ext, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m-> 4461\u001b[0m (y, zf) \u001b[38;5;241m=\u001b[39m \u001b[43msosfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4462\u001b[0m y_0 \u001b[38;5;241m=\u001b[39m axis_slice(y, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   4463\u001b[0m (y, zf) \u001b[38;5;241m=\u001b[39m sosfilt(sos, axis_reverse(y, axis\u001b[38;5;241m=\u001b[39maxis), axis\u001b[38;5;241m=\u001b[39maxis, zi\u001b[38;5;241m=\u001b[39mzi \u001b[38;5;241m*\u001b[39m y_0)\n",
      "File \u001b[0;32m/workspaces/UQ_Early_prediction_MI_BCI/.conda/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4345\u001b[0m, in \u001b[0;36msosfilt\u001b[0;34m(sos, x, axis, zi)\u001b[0m\n\u001b[1;32m   4343\u001b[0m zi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(np\u001b[38;5;241m.\u001b[39mreshape(zi, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_sections, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m   4344\u001b[0m sos \u001b[38;5;241m=\u001b[39m sos\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 4345\u001b[0m \u001b[43m_sosfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4346\u001b[0m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m x_shape\n\u001b[1;32m   4347\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(x, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from mne.decoding import CSP\n",
    "\n",
    "current_directory = os.path.abspath('')\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_directory, '..'))\n",
    "\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"ROOT:\", project_root)\n",
    "from Early_predict_UQ.data.make_dataset import make_data\n",
    "from Early_predict_UQ.data.plots import plot_accuracy_over_time_and_epochs, plot_confidence_over_time_and_epochs #, plot_cost_over_time_and_epochs\n",
    "\n",
    "#might be better to just look at the highest probability instead of comparing the 2 highest\n",
    "def early_pred(probabilities, predict, numTimesBelowThreshold, patience, current_n, predict_time):\n",
    "                probabilities = probabilities.flatten()\n",
    "                sorted_probs = sorted(probabilities, reverse=True)\n",
    "                #cost1 = 1/(1+(sorted_probs[0] - sorted_probs[1]))\n",
    "                confidence = 1 - (1/(1+(sorted_probs[0] + (sorted_probs[0] - sorted_probs[1]))))\n",
    "                if confidence < threshold and predict == False:\n",
    "                    numTimesBelowThreshold +=1\n",
    "                    if numTimesBelowThreshold == patience:\n",
    "                        predict = True\n",
    "                        predict_time = current_n\n",
    "                        print(\"BELOW\")\n",
    "                current_n+=1\n",
    "                return current_n, predict, predict_time, confidence\n",
    "\n",
    "# Setting hyper-parameters\n",
    "threshold = 0.5 # need to be a list with values between 0 and 1 to loop over\n",
    "patience = 2 # numTimesBelowThreshold - potential hyperparameter\n",
    "\n",
    "#subjects to consider\n",
    "subject_list = [1] # Choosing the subject or subjects\n",
    "# Preprocessed epochs\n",
    "epochs, labels = make_data(subject_list)\n",
    "# Asserting the epochs and labels (last row of the events matrix) to be used for the classification\n",
    "epochs_train = epochs.copy()\n",
    "labels = epochs.events[:, -1] - 4\n",
    "\n",
    "currentcv=0\n",
    "current_epoch = 0\n",
    "current_window = 0\n",
    "# Cross validation \n",
    "## (Might need to do cross session - session 1 as train, and session 2 as test. See dataset_structure.ipynb)\n",
    "scores = []\n",
    "epochs_data = epochs.get_data(copy=False)\n",
    "print(\"shape of epochs data:\\n\",epochs_data.shape)\n",
    "epochs_data_train = epochs_train.get_data(copy=False)\n",
    "cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "cv_split = cv.split(epochs_data_train)\n",
    "print(\"cv split: \\n\", cv_split)\n",
    "\n",
    "# LDA and CSP pipeline\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "csp = CSP(n_components=5, reg=None, log=True, norm_trace=False) # why 4 components or 5\n",
    "\n",
    "# Class balance between the 4 classes. \n",
    "class_balance = np.zeros(4)\n",
    "for i in range(4):\n",
    "    class_balance[i] = np.mean(labels == i)\n",
    "\n",
    "class_balance = np.max(class_balance)\n",
    "\n",
    "class_names = {\n",
    "        1: \"Left hand\",\n",
    "        2: \"Right hand\",\n",
    "        3: \"Both feet\",\n",
    "        4: \"Tongue\"\n",
    "}\n",
    "\n",
    "sfreq = 250 # Sampling frequency of 250 Hz as per the BCI competion dataset 2a\n",
    "\n",
    "# Classify the signal using a sliding window\n",
    "w_length = int(sfreq * 0.5)  # Window length\n",
    "w_step = int(sfreq * 0.1)  # window step size\n",
    "w_start = np.arange(0, epochs_data.shape[2] - w_length, w_step) # Set of starting positions in the signal(Note! the signal is 2s to 4s) \n",
    "#if len=0.5 and step = 0.1. Theres 36 starting points 0.1 s apart, multiplied by the sfreq\n",
    "\n",
    "scores_cv_splits = [] \n",
    "predict_time_cv_splits = []\n",
    "confidence_cv_split = []\n",
    "# Running classification across the signal\n",
    "for train_idx, test_idx in cv_split:\n",
    "    currentcv+=1\n",
    "    #print(\"train idx: \", train_idx)\n",
    "    #print(\"test idx: \", test_idx)\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx] # Get the current labels and data\n",
    "    # Exatract spatial filters and transform the data as a whole\n",
    "    X_train = csp.fit_transform(epochs_data_train[train_idx], y_train)\n",
    "    X_test = csp.transform(epochs_data_train[test_idx]) #  why define and transform it here, and then do it later as well!\n",
    "\n",
    "    # Fit the classifier on the training data\n",
    "    lda.fit(X_train, y_train)\n",
    "    w_times = (w_start + w_length / 2.0) / sfreq + epochs.tmin\n",
    "\n",
    "    # Test the classifier on the windows. This is where we run over the signal]\n",
    "    scores_across_epochs = []\n",
    "    confidence_across_epochs = []\n",
    "    predict_time_across_epochs = []\n",
    "        #Choose an epoch between 1 and 116. As the testset is 20% 116 epochs of the whole data 576 epochs\n",
    "    \n",
    "    numTimesBelowThreshold = 0\n",
    "    X_test = csp.transform(epochs_data[test_idx][:, :, n:(n + w_length)])  # window snippet across all epochs, but for a certian window\n",
    "    current_epoch = 0     \n",
    "    #Between 1 and 116. As the testset is 20% 116 epochs of the whole data 576 epochs\n",
    "    for epoch_idx in range(len(test_idx)): \n",
    "        current_n = 0\n",
    "        predict_time = 0\n",
    "        current_epoch+=1\n",
    "        predict = False \n",
    "        current_window = 0\n",
    "        for n in w_start:\n",
    "            current_window +=1 \n",
    "            print(f\" CV {currentcv}, {current_epoch}, and {current_window}\")\n",
    "            if not predict:\n",
    "                X_test_window = csp.transform(epochs_data[test_idx][:, :, n:(n + w_length)])\n",
    "                X_test_epoch_window = X_test[epoch_idx]\n",
    "\n",
    "                #Early prediction\n",
    "                probabilities = lda.predict_proba([X_test_epoch_window])\n",
    "                probabilities = np.array(probabilities)\n",
    "                probabilities = probabilities.flatten()\n",
    "                current_n, predict, predict_time, confidence = early_pred(probabilities, predict, numTimesBelowThreshold, patience, current_n, predict_time) # predict becomes true to go to the next loop\n",
    "            else:\n",
    "                break # predicting early\n",
    "        #if not predicted early, we still predict as we have reached the end of \n",
    "        score = lda.score(X_test_epoch_window.reshape(1, -1), [y_test[epoch_idx]])\n",
    "        y_test = y_test+4 # not realy useful?\n",
    "        scores_across_epochs.append(score)\n",
    "        predict_time_across_epochs.append(predict_time)\n",
    "        \n",
    "        print(\"scores_across_epochs (116?) :\", scores_across_epochs.shape)\n",
    "        print(\"predict_time_across_epochs (116?): \",predict_time_across_epochs.shape)\n",
    "        scores_cv_splits.append(scores_across_epochs)\n",
    "        predict_time_cv_splits.append(predict_time_across_epochs)\n",
    "\n",
    "print(\"scores_cv_splits (10X116?) :\",np.array(scores_across_epochs).shape)\n",
    "print(\"predict_time_cv_splits (10X116?): \",np.array(predict_time_across_epochs).shape)\n",
    "\n",
    "'''\n",
    "To do - dynamic stop:\n",
    "- sliding\n",
    "    - make the for loops work and contain and provide the mean predict time and score correctly\n",
    "    - then advance to for all subjects\n",
    "    - the start the hyperparameter tuning to maximize classification accuracy, and minimize predict_time \n",
    "    - then loop across all threshold values \n",
    "    - make it take into account all the subjects\n",
    "    - make it work using svm\n",
    "    - provide the plots for all the subjects for all subjects for each condition, let it just save the plots to a folder automatically (potentially also the values to make plots somewhere else)\n",
    "    - nb: watch the memory and time usage for codespaces\n",
    "- expanding:\n",
    "    - make a new file, adjust to use expanding window\n",
    "    - save its plots into another folder automaically\n",
    "            \n",
    "To do - static:\n",
    "- make a new file and adjust the dynamic to just use a specific predict times using the cost function\n",
    "- save the plots\n",
    "\n",
    "to do - whole:\n",
    "- already did that lol\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(\"len w-times: \", w_times)\n",
    "print(\"len w-start: \", w_start)\n",
    "print(\"len w-times[:numberOfNs]: \", w_times[:numberOfNs])\n",
    "print(\"len w-start[:numberOfNs]: \", w_start[:numberOfNs])\n",
    "print(\"number n's \", numberOfNs)\n",
    "'''\n",
    "\n",
    "''' ##Costs for each of the classes for each window\n",
    "plt.plot(w_times, confidences, label='Cost')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.axvline(w_times[predict_time], linestyle=\"-\", color=\"b\", label=\"Stopping\")\n",
    "plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "plt.axhline(0.5, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "plt.title(\"Cost over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##Accuracy for each window\n",
    "plt.plot(w_times, score_this_window, label=\"Score\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.axvline(w_times[predict_time], linestyle=\"-\", color=\"b\", label=\"Stopping\")\n",
    "plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "plt.axhline(class_balance, linestyle=\"-\", color=\"k\", label=\"Chance\")\n",
    "plt.title(\"Classification accuracy over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(w_times, confidence_this_window, label=\"Score\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.ylim(0,1)\n",
    "plt.axvline(w_times[predict_time], linestyle=\"-\", color=\"b\", label=\"Stopping\")\n",
    "plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "#plt.axhline(threshold, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "plt.title(\"Model confindence over Time\")\n",
    "plt.legend()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Increasing window instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Want to look at 1 epoch only, not all epochs in the test idx at the given sliding window\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def find_mean_probabilities(probabilities):\n",
    "   return np.mean(probabilities, axis=0)\n",
    "\n",
    "def calculate_max_mean_probability_and_class(probabilities):\n",
    "    max_mean_probability_index = np.argmax(np.mean(probabilities, axis=0))\n",
    "    max_mean_probability = np.max(np.mean(probabilities, axis=0))\n",
    "    corresponding_class = max_mean_probability_index + 1  # Classes are typically indexed starting from 1\n",
    "    return max_mean_probability, corresponding_class\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    " \n",
    "    return num\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "subject_list = [1] # Choosing the subject or subjects\n",
    "# Preprocessed epochs\n",
    "epochs, labels = make_data(subject_list)\n",
    "# Asserting the epochs and labels (last row of the events matrix) to be used for the classification\n",
    "epochs_train = epochs.copy()\n",
    "labels = epochs.events[:, -1] - 4\n",
    "\n",
    "# Cross validation \n",
    "## (Might need to do cross session - session 1 as train, and session 2 as test. See dataset_structure.ipynb)\n",
    "scores = []\n",
    "epochs_data = epochs.get_data(copy=False)\n",
    "print(\"shape of epochs data:\\n\",epochs_data.shape)\n",
    "epochs_data_train = epochs_train.get_data(copy=False)\n",
    "cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "cv_split = cv.split(epochs_data_train)\n",
    "print(\"cv split: \\n\", cv_split)\n",
    "\n",
    "# LDA and CSP pipeline\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "csp = CSP(n_components=5, reg=None, log=True, norm_trace=False) # why 4 components or 5\n",
    "\n",
    "# Class balance between the 4 classes. \n",
    "class_balance = np.zeros(4)\n",
    "for i in range(4):\n",
    "    class_balance[i] = np.mean(labels == i)\n",
    "\n",
    "class_balance = np.max(class_balance)\n",
    "\n",
    "sfreq = 250 # Sampling frequency of 250 Hz as per the BCI competion dataset 2a\n",
    "\n",
    "# Classify the signal using a growing window\n",
    "# Define initial window parameters\n",
    "initial_window_length = int(sfreq * 0.5)  # Initial window length\n",
    "w_step = int(sfreq * 0.1)  # Window step size\n",
    "w_start = np.arange(0, epochs_data.shape[2] - initial_window_length, w_step)  # Set of starting positions in the signal (Note! the signal is 2s to 4s)\n",
    "\n",
    "print(\"w start shape: \", w_start.shape)\n",
    "#print(\"w start: \\n\", w_start)\n",
    "scores_windows = [] \n",
    "#threshold = 0.5\n",
    "entropy_windows = []\n",
    "# Running classification across the signal\n",
    "for train_idx, test_idx in cv_split:\n",
    "    #print(\"train idx: \", train_idx)\n",
    "    #print(\"test idx: \", test_idx)\n",
    "    print(\"nr train_index:\", len(train_idx))\n",
    "    print(\"nr test_index:\", len(test_idx))\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx] # Get the current labels and data\n",
    "    # Exatract spatial filters and transform the data as a whole\n",
    "    X_train = csp.fit_transform(epochs_data_train[train_idx], y_train)\n",
    "    X_test = csp.transform(epochs_data_train[test_idx]) #  why define and transform it here, and then do it later as well!\n",
    "\n",
    "    # Fit the classifier on the training data\n",
    "    lda.fit(X_train, y_train)\n",
    "    w_times = (w_start + w_length / 2.0) / sfreq + epochs.tmin\n",
    "\n",
    "\n",
    "\n",
    "    # Test the classifier on the windows. This is where we run over the signal\n",
    "    preds_this_window = []\n",
    "    probs_this_window = []\n",
    "    score_this_window = []\n",
    "    entropy_this_window = []\n",
    "    confidence_this_window = []\n",
    "    chosen_epoch = 18 #Choose an epoch between 1 and 116. As the testset is 20% 116 epochs of the whole data 576 epochs\n",
    "    for n, window_start in enumerate(w_start):\n",
    "        window_length = initial_window_length + n * w_step\n",
    "        X_test  = csp.transform(epochs_data[test_idx][:, :, window_start:(window_start + window_length)])\n",
    "        print(\"X_test  shape:\\n\",X_test.shape)\n",
    "        X_test_1_epoch = X_test [chosen_epoch] #Chooosing a specific epoch in the test set \n",
    "        print(\"X_test_1_epoch shape:\\n\", X_test_1_epoch.shape)\n",
    "\n",
    "        #Accuracy\n",
    "        score = lda.score(X_test_1_epoch.reshape(1, -1), [y_test[chosen_epoch]])\n",
    "        score_this_window.append(score)\n",
    "        \n",
    "        probabilities = lda.predict_proba([X_test_1_epoch])\n",
    "        \n",
    "        if len(probs_this_window) == 0:\n",
    "            probs_this_window = probabilities\n",
    "        else:\n",
    "            probs_this_window = np.vstack((probs_this_window, probabilities))\n",
    "\n",
    "        prediction = lda.predict([X_test_1_epoch])\n",
    "        preds_this_window.append(prediction)\n",
    "\n",
    "        print(\"Prediction for this time window: \", prediction)\n",
    "        print(\"prob shape: \", probabilities.shape)\n",
    "        print(\"probabilities: \\n\", probabilities)\n",
    "\n",
    "        #predictive entropy - H_pred(p) \n",
    "        entropy_score = entropy(probabilities, axis = 1) #- see if entropy is better than probabilites\n",
    "        entropy_this_window.append(entropy_score)\n",
    "\n",
    "        '''\n",
    "        Confidence - as seen in: \n",
    "        Uncertainty Quantification in Machine Learning for Biosignal Applications - A Review, page 13.\n",
    "        1 - H_pred(p) can be used as a confidence measure. Normalizing seems useful - 1 / (1- entropy-score)\n",
    "        '''\n",
    "        #confidence\n",
    "        confidence = 1 - entropy_score\n",
    "        confidence_this_window.append(confidence)\n",
    "\n",
    "    class_names = {\n",
    "        1: \"Left hand\",\n",
    "        2: \"Right hand\",\n",
    "        3: \"Both feet\",\n",
    "        4: \"Tongue\"\n",
    "    }\n",
    "\n",
    "    ##Probabiltiies for each of the classes for each window\n",
    "    plt.plot(w_times, probs_this_window, label=[class_names[label] for label in [1, 2, 3, 4]])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Probabilities\")\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.axhline(0.5, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "    plt.title(\"Classification probabilities over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ##Predictions for each window\n",
    "    preds_this_window_plottable = [x+4 for x in preds_this_window]\n",
    "    plt.plot(w_times, preds_this_window_plottable)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Class prediction\")\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.title(\"Classification over Time\")\n",
    "    plt.yticks([1, 2, 3, 4], [class_names[label] for label in [1, 2, 3, 4]])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "\n",
    "    ##Accuracy for each window\n",
    "    plt.plot(w_times, score_this_window, label=\"Score\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.axhline(class_balance, linestyle=\"-\", color=\"k\", label=\"Chance\")\n",
    "    plt.title(\"Classification accuracy over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ##Entropy for each window \n",
    "    plt.plot(w_times, entropy_this_window, label=\"Score\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Entropy \")\n",
    "    plt.ylim(0,1)\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    plt.title(\"Model entropy over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(w_times, confidence_this_window, label=\"Score\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Confidence\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.axvline(2, linestyle=\"--\", color=\"k\", label=\"Onset\")\n",
    "    #plt.axhline(threshold, linestyle=\"-\", color=\"k\", label=\"Threshold\")\n",
    "    plt.title(\"Model confidence over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    y_test = y_test+4\n",
    "    print(\"right label:\", class_names[y_test[chosen_epoch]])\n",
    "    break\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
