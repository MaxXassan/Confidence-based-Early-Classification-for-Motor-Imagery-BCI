{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameter tuning - highest_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "from mne.decoding import CSP\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "current_directory = os.path.abspath('')\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_directory, '..', '..', '..', '..'))\n",
    "\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"ROOT:\", project_root)\n",
    "from Early_predict_UQ.data.make_dataset import make_data\n",
    "\n",
    "\n",
    "# epoch tmin  = 2 and tmax = 6 , as the motor imagery task lasted in that time\n",
    "def early_pred(probabilities, predict, numTimesBelowThreshold, patience, confidence_type, threshold):\n",
    "    probabilities = probabilities.flatten()\n",
    "    sorted_probs = sorted(probabilities, reverse=True)\n",
    "    if confidence_type == 'highest_prob':\n",
    "        confidence = sorted_probs[0]\n",
    "    else:\n",
    "        confidence = 1 - (1 / (1 + (sorted_probs[0] + (sorted_probs[0] - sorted_probs[1]))))\n",
    "    if confidence > threshold and not predict:\n",
    "        #print(\"confindence:\", confidence)\n",
    "        sorted_probs[0]\n",
    "        numTimesBelowThreshold += 1\n",
    "        if numTimesBelowThreshold == patience:\n",
    "            predict = True\n",
    "    return predict, confidence, numTimesBelowThreshold\n",
    "\n",
    "def run_sliding_classification(subjects, threshold, patience, confidence_type, w_length, w_step, sfreq):\n",
    "    scores_across_subjects = []\n",
    "    prediction_time_across_subjects = []\n",
    "    current_person = 0\n",
    "    for person in subjects:\n",
    "        current_person += 1\n",
    "        print(\"Person %d\" % (person))\n",
    "        subject= [person]\n",
    "        epochs, labels = make_data(subject)\n",
    "        epochs_train = epochs.copy()\n",
    "        labels = epochs.events[:, -1] - 4\n",
    "        epochs_data = epochs.get_data(copy=False)\n",
    "        epochs_data_train = epochs_train.get_data(copy=False)\n",
    "\n",
    "        cv = ShuffleSplit(3, test_size=0.2, random_state=42)\n",
    "        cv_split = cv.split(epochs_data_train)\n",
    "        scores_cv_splits = []\n",
    "        predict_time_cv_splits = []\n",
    "\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "        current_cv = 0 \n",
    "        for train_idx, test_idx in cv_split:\n",
    "            current_cv += 1\n",
    "            y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "            X_train = csp.fit_transform(epochs_data_train[train_idx], y_train)\n",
    "            lda.fit(X_train, y_train)\n",
    "            w_start = np.arange(0, epochs_data.shape[2] - w_length, w_step) \n",
    "            scores_across_epochs = []\n",
    "            predict_time_across_epochs = []\n",
    "\n",
    "            for epoch_idx in range(len(test_idx)):\n",
    "                predict = False\n",
    "                numTimesBelowThreshold = 0\n",
    "                for n in w_start:\n",
    "                    X_test_window = csp.transform(epochs_data_train[test_idx][:, :, n:(n + w_length)])\n",
    "                    X_test_epoch_window = X_test_window[epoch_idx]\n",
    "                    probabilities = lda.predict_proba([X_test_epoch_window])\n",
    "                    probabilities = np.array(probabilities)\n",
    "                    probabilities = probabilities.flatten()\n",
    "                    predict, confidence, numTimesBelowThreshold = early_pred(\n",
    "                        probabilities, predict, numTimesBelowThreshold, patience, confidence_type, threshold\n",
    "                    )\n",
    "                    if predict:\n",
    "                        #IF WE DIDNT PREDICT EARLY, MAYBE PREDICT ON THE WHOLE EPOCH?\n",
    "                        predict_time = n\n",
    "                        score = lda.score(X_test_epoch_window.reshape(1, -1), [y_test[epoch_idx]])\n",
    "                        break\n",
    "                else:\n",
    "                    predict_time = n\n",
    "                    score = lda.score(X_test_epoch_window.reshape(1, -1), [y_test[epoch_idx]])\n",
    "                predict_time = (predict_time + w_length / 2.0) / sfreq + epochs.tmin\n",
    "                scores_across_epochs.append(score)\n",
    "                predict_time_across_epochs.append(predict_time)\n",
    "\n",
    "            if current_cv == 1:\n",
    "                scores_cv_splits = np.array(scores_across_epochs)\n",
    "                predict_time_cv_splits = np.array(predict_time_across_epochs)\n",
    "            else:\n",
    "                scores_cv_splits = np.vstack((scores_cv_splits,np.array(scores_across_epochs)))\n",
    "                predict_time_cv_splits = np.vstack((predict_time_cv_splits,np.array(predict_time_across_epochs)))\n",
    "\n",
    "        mean_scores_across_cv = np.mean(scores_cv_splits, axis=0)\n",
    "        mean_predict_time_across_cv = np.mean(predict_time_cv_splits, axis=0)\n",
    "        if current_person == 1:\n",
    "            scores_across_subjects  = np.array(mean_scores_across_cv)\n",
    "            prediction_time_across_subjects = np.array(mean_predict_time_across_cv)\n",
    "        else:\n",
    "            scores_across_subjects = np.vstack((scores_across_subjects,np.array(mean_scores_across_cv)))\n",
    "            prediction_time_across_subjects = np.vstack((predict_time_cv_splits,np.array(mean_predict_time_across_cv)))\n",
    "\n",
    "        mean_scores_across_subjects = np.mean(scores_across_subjects, axis=0)\n",
    "        accuracy = np.mean(mean_scores_across_subjects)\n",
    "\n",
    "        mean_prediction_time_across_subjects = np.mean(prediction_time_across_subjects, axis=0)\n",
    "        mean_prediction_time = np.mean(mean_prediction_time_across_subjects)\n",
    "    return accuracy, mean_prediction_time, epochs, labels\n",
    " \n",
    "def create_parameterslist():\n",
    "    rng = np.random.RandomState(42)\n",
    "    # Generate random values for w_length and w_step\n",
    "    w_length_values = np.round(rng.uniform(0.1, 1, 10), 2)\n",
    "    w_step_values = []\n",
    "\n",
    "    for w_length in w_length_values:\n",
    "        # Generate a random step between 0.1 and the value of w_length\n",
    "        w_step = np.round(rng.uniform(0.1, w_length, 1)[0], 2)\n",
    "        w_step_values.append(w_step)\n",
    "\n",
    "    parameters = {\n",
    "        'csp_components': [2, 4, 6, 8, 10], \n",
    "        'w_length': w_length_values, \n",
    "        'w_step': w_step_values\n",
    "    }\n",
    "\n",
    "    parameters_list = list(ParameterSampler(parameters, n_iter=10, random_state=rng))\n",
    "    return parameters_list\n",
    "\n",
    "def hyperparameter_tuning (parameters_list):\n",
    "\n",
    "    for n, param_set in enumerate(parameters_list):\n",
    "        csp_components = param_set['csp_components']\n",
    "        initial_window_length = int(sfreq * param_set['initial_window_length'])  \n",
    "        expansion_rate = int(sfreq * param_set['expansion_rate'])\n",
    "\n",
    "        subjects_accuracies, accuracy = run_sliding_classification(subjects, initial_window_length, expansion_rate, csp_components)\n",
    "        \n",
    "        mean_accuracy = np.mean(accuracy)\n",
    "\n",
    "        print(f\"Iteration {n+1}/{len(parameters_list)}: Mean accuracy for parameters {param_set} is {mean_accuracy}\")\n",
    "\n",
    "        if mean_accuracy > best_accuracy:\n",
    "            best_accuracy = mean_accuracy\n",
    "            best_params = param_set\n",
    "\n",
    "    return best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subjects = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # 9 subjects\n",
    "    sfreq = 250      \n",
    "    epochs, labels = make_data([1]) # just to access the epoch.shape\n",
    "    epochs_data = epochs.get_data(copy=False)\n",
    "\n",
    "    #Hyperparameter_tuning\n",
    "    print(\"Hyperparameter tuning: \\n\\n\")\n",
    "    parameters_list = create_parameterslist()\n",
    "    best_params = hyperparameter_tuning(parameters_list)\n",
    "    \n",
    "    print(\"\\n\\n Classification: \\n\\n\")\n",
    "    subjects_accuracies, accuracy, epochs, labels = run_expanding_classification(subjects, best_params['initial_window_length'], best_params['expansion_rate'], best_params['csp_components'])\n",
    "    accuracy_array = np.array(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "To do - dynamic stop:\n",
    "- sliding\n",
    "    - make the for loops work and contain and provide the mean predict time and score correctly X\n",
    "    - then advance to for all subjects X\n",
    "    - modularize\n",
    "    - the start the hyperparameter tuning to maximize classification accuracy, and minimize predict_time \n",
    "    - then loop across all threshold values \n",
    "    - make it take into account all the subjects\n",
    "    - make it work using svm\n",
    "    - provide the plots for all the subjects for all subjects for each condition, let it just save the plots to a folder automatically (potentially also the values to make plots somewhere else)\n",
    "    - nb: watch the memory and time usage for codespaces\n",
    "- expanding:\n",
    "    - make a new file, adjust to use expanding window\n",
    "    - save its plots into another folder automaically\n",
    "            \n",
    "To do - static:\n",
    "- make a new file and adjust the dynamic to just use a specific predict times using the cost function\n",
    "- save the plots\n",
    "\n",
    "to do - whole:\n",
    "- already did that \n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_length_values: [0.44 0.96 0.76 0.64 0.24 0.24 0.15 0.88 0.64 0.74]\n",
      "w_step_values: [0.11, 0.93, 0.65, 0.21, 0.13, 0.13, 0.12, 0.51, 0.33, 0.29]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate random values for w_length and w_step\n",
    "w_length_values = np.round(rng.uniform(0.1, 1, 10), 2)\n",
    "w_step_values = []\n",
    "\n",
    "for w_length in w_length_values:\n",
    "    # Generate a random step between 0.1 and the value of w_length\n",
    "    w_step = np.round(rng.uniform(0.1, w_length, 1)[0], 2)\n",
    "    w_step_values.append(w_step)\n",
    "\n",
    "print(\"w_length_values:\", w_length_values)\n",
    "print(\"w_step_values:\", w_step_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter tuning - cost func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from mne.decoding import CSP\n",
    "\n",
    "current_directory = os.path.abspath('')\n",
    "project_root = os.path.abspath(os.path.join(current_directory, '..', '..', '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "print(\"ROOT:\", project_root)\n",
    "from Early_predict_UQ.data.make_dataset import make_data\n",
    "\n",
    "def early_pred(probabilities, predict, numTimesBelowThreshold, patience, confidence_type, threshold):\n",
    "    probabilities = probabilities.flatten()\n",
    "    sorted_probs = sorted(probabilities, reverse=True)\n",
    "    if confidence_type == 'highest_prob':\n",
    "        confidence = sorted_probs[0]\n",
    "    else:\n",
    "        confidence = 1 - (1 / (1 + (sorted_probs[0] + (sorted_probs[0] - sorted_probs[1]))))\n",
    "    if confidence > threshold and not predict:\n",
    "        numTimesBelowThreshold += 1\n",
    "        if numTimesBelowThreshold == patience:\n",
    "            predict = True\n",
    "    return predict, confidence, numTimesBelowThreshold\n",
    "\n",
    "def run_sliding_classification(subjects, threshold, patience, confidence_type, w_length, w_step, sfreq, n_components):\n",
    "    scores_across_subjects = []\n",
    "    prediction_time_across_subjects = []\n",
    "    for person in subjects:\n",
    "        print(\"Processing Person %d\" % (person))  # Print statement to track progress\n",
    "        subject= [person]\n",
    "        epochs, labels = make_data(subject)\n",
    "        epochs_train = epochs.copy()\n",
    "        labels = epochs.events[:, -1] - 4\n",
    "        epochs_data = epochs.get_data(copy=False)\n",
    "        epochs_data_train = epochs_train.get_data(copy=False)\n",
    "\n",
    "        cv = ShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "        cv_split = cv.split(epochs_data_train)\n",
    "        scores_cv_splits = []\n",
    "        predict_time_cv_splits = []\n",
    "\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        csp = CSP(n_components=n_components, reg=None, log=True, norm_trace=False)\n",
    "        for train_idx, test_idx in cv_split:\n",
    "            y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "            X_train = csp.fit_transform(epochs_data_train[train_idx], y_train)\n",
    "            lda.fit(X_train, y_train)\n",
    "            w_start = np.arange(0, epochs_data.shape[2] - w_length, w_step)\n",
    "            scores_across_epochs = []\n",
    "            predict_time_across_epochs = []\n",
    "\n",
    "            for epoch_idx in range(len(test_idx)):\n",
    "                predict = False\n",
    "                numTimesBelowThreshold = 0\n",
    "                for n in w_start:\n",
    "                    X_test_window = csp.transform(epochs_data_train[test_idx][:, :, n:(n + w_length)])\n",
    "                    X_test_epoch_window = X_test_window[epoch_idx]\n",
    "                    probabilities = lda.predict_proba([X_test_epoch_window])\n",
    "                    probabilities = np.array(probabilities)\n",
    "                    probabilities = probabilities.flatten()\n",
    "                    predict, confidence, numTimesBelowThreshold = early_pred(\n",
    "                        probabilities, predict, numTimesBelowThreshold, patience, confidence_type, threshold\n",
    "                    )\n",
    "                    if predict:\n",
    "                        predict_time = n\n",
    "                        score = lda.score(X_test_epoch_window.reshape(1, -1), [y_test[epoch_idx]])\n",
    "                        break\n",
    "                else:\n",
    "                    predict_time = n\n",
    "                    score = lda.score(X_test_epoch_window.reshape(1, -1), [y_test[epoch_idx]])\n",
    "                predict_time = (predict_time + w_length / 2.0) / sfreq + epochs.tmin\n",
    "                scores_across_epochs.append(score)\n",
    "                predict_time_across_epochs.append(predict_time)\n",
    "\n",
    "            scores_cv_splits.append(np.mean(scores_across_epochs))\n",
    "            predict_time_cv_splits.append(np.mean(predict_time_across_epochs))\n",
    "\n",
    "        mean_scores_across_cv = np.mean(scores_cv_splits)\n",
    "        mean_predict_time_across_cv = np.mean(predict_time_cv_splits)\n",
    "\n",
    "        scores_across_subjects.append(mean_scores_across_cv)\n",
    "        prediction_time_across_subjects.append(mean_predict_time_across_cv)\n",
    "\n",
    "    accuracy = np.mean(scores_across_subjects)\n",
    "    mean_prediction_time = np.mean(prediction_time_across_subjects)\n",
    "\n",
    "    return accuracy, mean_prediction_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threshold = 0.4  # values - {0,1}\n",
    "    patience = 4  # values - {1,36}\n",
    "    subjects = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # 9 subjects\n",
    "    confidence_type = 'cost'  # 'highest_prob' or the cost function. # hyperparameter - maybe compare in different files\n",
    "    sfreq = 250\n",
    "    w_length = int(sfreq * 0.5)\n",
    "    w_step = int(sfreq * 0.5)\n",
    "\n",
    "    # Define hyperparameter values\n",
    "    n_components_values = [2, 4, 6, 8, 10]\n",
    "    w_length_values = [int(sfreq * 0.1), int(sfreq * 0.5), int(sfreq * 1)]\n",
    "    w_step_values = [int(sfreq * 0.1), int(sfreq * 0.5), int(sfreq * 1)]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_components in n_components_values:\n",
    "        for w_length in w_length_values:\n",
    "            for w_step in w_step_values:\n",
    "                print(\"Evaluating: n_components={}, w_length={}, w_step={}\".format(n_components, w_length, w_step))  # Print statement to track progress\n",
    "                accuracy, mean_prediction_time = run_sliding_classification(subjects, threshold, patience, confidence_type, w_length, w_step, sfreq, n_components)\n",
    "                results.append((n_components, w_length, w_step, accuracy, mean_prediction_time))\n",
    "\n",
    "    # Print results\n",
    "    for result in results:\n",
    "        print(\"n_components: {}, w_length: {}, w_step: {}, accuracy: {}, mean_prediction_time: {}\".format(*result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
